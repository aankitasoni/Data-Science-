# What is Data Science?

* Data Science is the process and method for extracting knowledge and insights from large volumes of disparate data. It's an interdisciplinary field involving mathematics, statistical analysis, data visualization, machine learning, and more. It's what makes it possible for us to appropriate information, see patterns, find meaning from large volumes of data and use it to make decisions that drive business. Data Science can use many of the AI techniques to derive insight from data. For example, it could use machine learning algorithms and even deep learning models to extract meaning and draw inferences from data. 

* There is some interaction between AI and Data Science, but one is not a subset of the other. Rather, Data Science is a broad term that encompasses the entire data processing methodology while AI includes everything that allows computers to learn how to solve problems and make intelligent decisions. Both AI and Data Science can involve the use of big data.

* Data scientists can analyze structured and unstructured data from many sources, and depending on the nature of the problem, they can choose to analyze the data in different ways. Using multiple models to explore the data reveals patterns and outliers; sometimes, this will confirm what the organization suspects, but sometimes it will be completely new knowledge, leading the organization to a new approach.



# Terms Used in Data Science.

**1. Big Data** - The term big data refers to data sets that are so massive, so quickly built, and so varied that they defy traditional analysis methods such as you might perform with a relational database. The concurrent development of enormous compute power in distributed networks and new tools and techniques for data analysis means that organizations now have the power to analyze these vast data sets. A new knowledge and insights are becoming available to everyone.

**2. Data Mining** - It is the process of automatically searching and analyzing data, discovering previously unrevealed patterns. It involves preprocessing the data to prepare it and transforming it into an appropriate format. Once this is done, insights and patterns are mined and extracted using various tools and techniques ranging from simple data visualization tools to machine learning and statistical models.

**3.  Machine learning** - It is a subset of AI that uses computer algorithms to analyze data and make intelligent decisions based on what it is learned without being explicitly programmed. Machine learning algorithms are trained with large sets of data and they learn from examples. They do not follow rules-based algorithms. Machine learning is what enables machines to solve problems on their own and make accurate predictions using the provided data.

**4. Deep learning** - It is a specialized subset of machine learning that uses layered neural networks to simulate human decision-making. Deep learning algorithms can label and categorize information and identify patterns. It is what enables AI systems to continuously learn on the job and improve the quality and accuracy of results by determining whether decisions were correct. 

**5. Artificial Neural Networks** - A neural network in AI is a collection of small computing units called neurons that take incoming data and learn to make decisions over time. Neural networks are often layer-deep and are the reason deep learning algorithms become more efficient as the data sets increase in volume, as opposed to other machine learning algorithms that may plateau as data increases.


# Advice to the New Data Scientists.

An aspiring data scientist is to be curious, extremely argumentative and judgmental. 

  - Curiosity is absolute must. If you're not curious, you would not know what to do with the data. 
  - Judgmental because if you do not have preconceived notions about things you wouldn't know where to begin with. 
  - Argumentative because if you can argument and if you can plead a case, at least you can start somewhere and then you learn from data and then you modify your assumptions and hypotheses and your data would help you learn.



# Role of Cloud for Data Science.

* Cloud is a godsend for data scientists. It allows us to bypass the physical limitations of the computers and the systems we're using and allows us to deploy the analytics and storage capacities of advanced machines that do not necessarily have to be our machine or our company's machine. 

* Cloud have the algorithms available, so we deploy those algorithms on very large datasets and we're able to do it even though our own systems, our own machines, our own computing environments were not allowing us to do so.

* Using the Cloud enables us to get instant access to open source technologies like Apache Spark without the need to install and configure them locally. Using the Cloud also gives us access to the most up-to-date tools and libraries without the worry of maintaining them and ensuring that they are up to date. 

* The Cloud is accessible from everywhere and in every time zone.



# Foundations of Big Data.

### **What is Big Data?**
 “Big Data refers to the dynamic, large and disparate volumes of data being created by people, tools, and machines. It requires new, innovative, and scalable technology to collect, host, and analytically process the vast amount of data gathered in order to derive real-time business insights that relate to consumers, risk, profit, performance, productivity management, and enhanced shareholder value.” - Ernst and Young
 
 #### There are V's of Big Data:
 * **Velocity** - Velocity is the speed at which data accumulates. Data is being generated extremely fast, in a process that never stops. Near or real-time streaming, local, and cloud-based technologies can process information very quickly.

 * **Volume** - Volume is the scale of the data, or the increase in the amount of data stored. Drivers of volume are the increase in data sources, higher resolution sensors, and scalable infrastructure.
 
 * **Variety** - Variety is the diversity of the data. Structured data fits neatly into rows and columns, in relational databases while unstructured data is not organized in a pre-defined way, like Tweets, blog posts, pictures, numbers, and video. 
 
    Variety also reflects that data comes from different sources, machines, people, and processes, both internal and external to organizations. Drivers are mobile technologies, social media, wearable technologies, geo technologies, video, and many, many more. 
 
 * **Veracity** - Veracity is the quality and origin of data, and its conformity to facts and accuracy. 
     - Attributes include consistency, completeness, integrity, and ambiguity. 
     - Drivers include cost and the need for traceability. 
 
       With the large amount of data available, the debate rages on about the accuracy of data in the digital age. Is the information real, or is it false?
    
        80% of data is considered to be unstructured and we must devise ways to produce reliable and accurate insights. The data must be categorized, analyzed, and visualized. 
 
 * **Value** - Value is our ability and need to turn data into value. Value isn't just profit. It may have medical or social benefits, as well as customer, employee, or personal satisfaction. The main reason that people invest time to understand Big Data is to derive value from it.


    So the scale of the data being collected means that it’s not feasible to use conventional data analysis tools. However, alternative tools that leverage distributed computing power can overcome this problem. Tools such as: 
    * **Apache Spark** (Open source unified analytics engine for large scale data processing), 
    * **Hadoop** (Open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data),
 
     and its ecosystem provide ways to extract, load, analyze, and process the data across distributed compute resources, providing new insights and knowledge. 

     This gives organizations more ways to connect with their customers and enrich the services they offer.


### What is Data Mining?

Data Mining is the process of extracting usable data from a large set of any raw data. The first step in data mining requires you to set up goals for the exercise. i.e.,

* **Selecting Data** as the output of a data-mining exercise largely depends upon the quality of data being used. The type of data, its size, and frequency of collection have a direct bearing on the cost of data mining exercise.

* **Preprocessing Data** as tha quality of the data should be checked before applying machine learning or data mining algorithms because we cannot work with raw data.

* **Transforming Data** which converts the raw data into a suitable format that efficiently eases data mining and retrives strategic information.

* **Storing Data** as the data must be stored in a format that gives unrestricted and immediate read/write privileges to the data scientist. Data safety and privacy should be a prime concern for storing data on servers or storage media that keeps the data secure and also prevents the data mining algorithm from unnecessarily searching for pieces of data scattered on different servers or storage media.

* **Mining Data** so it helps in developing a preliminary understanding of the trends hidden in the data set. A good starting point for data mining is data visualization.

* **Evaluating Mining Results** as it's an iterative process such that the analysts use better and improved algorithms to improve the quality of results generated in light of the feedback received from the key stakeholders.
    


# How Data Science Useful?

### Data Science in Healthcare.

- Data scientists use predictive analytics developed from data mining, data modeling, statistics, and machine learning to find the best options for patients.

- Data science systems that use predictive analytics ensure that all physicians can also access the latest information about the disease, tests, and treatment plans, tailored to their specific patient.

### Data Science Careers.

* Data Scientists need programming, mathematics, and database skills, many of which can be gained through self-learning.

* Companies recruiting for a Data Science team need to understand the variety of different roles Data Scientists can play, and look for soft skills like storytelling and relationship building as well as technical skills.

* High school students considering a career in Data Science should learn programming, math, databases, and, most importantly practice their skills.


# The Report Structure.

The structure of the report depends on the length of the document. 
- A brief report is more to the point and presents a summary of key findings. Brief reports were drafted as commentaries on current trends and developments that attracted public or media attention. 
- A detailed report incrementally builds the argument and contains details about other relevant works, research methodology, data sources, and intermediate findings along with the main results. Detailed and comprehensive reports offered a critical review of the subject matter with extensive data analysis and commentary. Often, detailed reports collected new data or interviewed industry experts to answer the research questions.

The report that is deliverable follow a prescribed format including the cover page, table of contents, executive summary, detailed contents, acknowledgments, references, and appendices (if needed).

- The cover page should include the title of the report, names of authors, their affiliations, and contacts, the name of the institutional publisher (if any), and the date of publication.
- The "Table of Contents" (ToC) with main headings and lists of tables and figures offers a glimpse of what lies ahead in the document.
- An "abstract" or an "executive summary" and an "introductory section" is always helpful in setting up the problem for the reader who might be new to the topic and who might need to be gently introduced to the subject matter before being immersed in intricate details. 
- In the "methodology" section, introduce the research methods and data sources used for the analysis. If new data collected, explain the data collection exercise in some detail.
- The results section is where you present your empirical findings. Starting with descriptive statistics and illustrative graphics, move toward formally testing your hypothesis.
- The results section is followed by the discussion section, where you craft your main arguments by building on the results you have presented earlier.
- In the "conclusion" section, generalize specific findings and take on a rather marketing approach to promote findings so that the reader does not remain stuck in the caveats that have voluntarily outlined earlier and also identify future possible developments in research and applications that could result from the research.
























